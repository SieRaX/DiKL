# Diffusive KL Divergence (DiKL)
Official PyTorch implementation of paper [Training Neural Samplers with Reverse Diffusive KL Divergence](https://arxiv.org/abs/2410.12456).

## Reproducing results for DiKL and Baselines
We provide samples on MoG-40, MW-32, DW-4, and LJ-13 for our methods and baselines (iDEM, FAB, reverse KL) in the following for rapid reproduction of our results.
